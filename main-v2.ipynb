{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee9082eb-10ae-4da6-ad24-ff8ba1f99bc9",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'airflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# I dont have airflow installed locally so i will see what todo\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mairflow\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DAG\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mairflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mproviders\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mstandard\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01moperators\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PythonOperator\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mairflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mproviders\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mstandard\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01moperators\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mempty\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m EmptyOperator\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'airflow'"
     ]
    }
   ],
   "source": [
    "# I dont have airflow installed locally so i will see what todo\n",
    "from airflow import DAG\n",
    "from airflow.providers.standard.operators.python import PythonOperator\n",
    "from airflow.providers.standard.operators.empty import EmptyOperator\n",
    "from airflow.providers.amazon.aws.hooks.s3 import S3Hook\n",
    "from airflow.providers.snowflake.operators.snowflake import SQLExecuteQueryOperator\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "import yfinance as yf # Theres a lib I have to install for this yfinance thing\n",
    "import requests\n",
    "import os\n",
    "\n",
    "# -----------------------------------------\n",
    "# Constants\n",
    "# -----------------------------------------\n",
    "SP500_URL = 'https://en.wikipedia.org/wiki/List_of_S%26P_500_companies'\n",
    "S3_BUCKET = 'sp500-stock-data'   \n",
    "S3_FOLDER = 'stock'\n",
    "\n",
    "# Data processing settings\n",
    "MAX_SYMBOLS_FOR_TESTING = None  # Set to None for all symbols\n",
    "DATA_RETENTION_DAYS = 1       # How many days of historical data to fetch\n",
    "\n",
    "# -----------------------------------------\n",
    "# Functions\n",
    "# -----------------------------------------\n",
    "def extract_symbols(ti, **kwargs):\n",
    "    \"\"\"Extract S&P 500 symbols from Wikipedia with enhanced validation\"\"\"\n",
    "    try:\n",
    "        print(\"Extracting S&P 500 symbols from Wikipedia...\")\n",
    "\n",
    "        headers = {\n",
    "            \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "        }\n",
    "\n",
    "        response = requests.get(SP500_URL, headers=headers, timeout=30)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        tables = pd.read_html(response.text)\n",
    "\n",
    "        if not tables:\n",
    "            raise ValueError(\"No tables found in the Wikipedia page\")\n",
    "\n",
    "        # Get the first table (S&P 500 companies)\n",
    "        df = tables[0]\n",
    "\n",
    "        # Validate required columns\n",
    "        if 'Symbol' not in df.columns:\n",
    "            raise ValueError(\"Symbol column not found in the table\")\n",
    "\n",
    "        # Clean and validate symbols\n",
    "        symbols = df['Symbol'].dropna().tolist()\n",
    "        symbols = [symbol.strip() for symbol in symbols if symbol.strip()]\n",
    "\n",
    "        # Remove any invalid symbols \n",
    "        valid_symbols = []\n",
    "        for symbol in symbols:\n",
    "            if len(symbol) <= 5 and symbol.replace('.', '').replace('-', '').isalnum():\n",
    "                valid_symbols.append(symbol)\n",
    "            else:\n",
    "                print(f\"Skipping invalid symbol: {symbol}\")\n",
    "\n",
    "        print(f\"Extracted {len(valid_symbols)} valid symbols from {len(symbols)} total\")\n",
    "\n",
    "        if not valid_symbols:\n",
    "            raise ValueError(\"No valid symbols found\")\n",
    "\n",
    "        # For testing, limit symbols. Set MAX_SYMBOLS_FOR_TESTING to None for production\n",
    "        if MAX_SYMBOLS_FOR_TESTING:\n",
    "            symbols_to_process = valid_symbols[:MAX_SYMBOLS_FOR_TESTING]\n",
    "        else:\n",
    "            symbols_to_process = valid_symbols\n",
    "        print(f\"Processing {len(symbols_to_process)} symbols for this run\")\n",
    "\n",
    "        # Send to XCom\n",
    "        ti.xcom_push(key='symbols', value=symbols_to_process)\n",
    "        ti.xcom_push(key='total_symbols_found', value=len(valid_symbols))\n",
    "\n",
    "        return symbols_to_process\n",
    "\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Network error while fetching symbols: {str(e)}\")\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting symbols: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "def get_stock_data(**context):\n",
    "    \"\"\"Fetch stock data for symbols with enhanced data quality\"\"\"\n",
    "    symbols = context['ti'].xcom_pull(key='symbols', task_ids='extract_symbols')\n",
    "    all_data = []\n",
    "\n",
    "    print(f\"Processing {len(symbols)} symbols...\")\n",
    "\n",
    "    for symbol in symbols:\n",
    "        try:\n",
    "            print(f\"Fetching data for {symbol}...\")\n",
    "\n",
    "            # Download data with better error handling\n",
    "            df = yf.download(\n",
    "                tickers=symbol,\n",
    "                start=datetime.now() - timedelta(days=DATA_RETENTION_DAYS),\n",
    "                end=datetime.now(),\n",
    "                interval=\"1d\",\n",
    "                progress=False,\n",
    "                auto_adjust=False,  # Explicitly set to avoid warning\n",
    "                prepost=False,       # Don't include pre/post market data\n",
    "                threads=True         # Use threading for better performance\n",
    "            )\n",
    "\n",
    "            if df.empty:\n",
    "                print(f\"No data available for {symbol}\")\n",
    "                continue\n",
    "\n",
    "            # Debug: Print DataFrame info\n",
    "            print(f\"{symbol} - DataFrame shape: {df.shape}\")\n",
    "            print(f\"{symbol} - Columns: {list(df.columns)}\")\n",
    "\n",
    "            # Handle multi-level columns properly\n",
    "            if isinstance(df.columns, pd.MultiIndex):\n",
    "                print(f\"{symbol} - MultiIndex columns detected, flattening...\")\n",
    "                # Flatten multi-level columns - keep only the first level (column name)\n",
    "                df.columns = [col[0] for col in df.columns]\n",
    "                print(f\"{symbol} - Flattened columns: {list(df.columns)}\")\n",
    "\n",
    "            # Reset index to convert Date from index to column\n",
    "            df = df.reset_index()\n",
    "            print(f\"{symbol} - After reset_index: {list(df.columns)}\")\n",
    "\n",
    "            # Ensure Date column exists and format it properly\n",
    "            if 'Date' in df.columns:\n",
    "                df['Date'] = pd.to_datetime(df['Date'], errors='coerce').dt.strftime('%Y-%m-%d')\n",
    "            else:\n",
    "                print(f\"No Date column found for {symbol}, available columns: {list(df.columns)}\")\n",
    "                continue\n",
    "\n",
    "            # Add symbol column\n",
    "            df[\"Symbol\"] = symbol\n",
    "\n",
    "            # Standardize column names (handle different naming conventions)\n",
    "            column_mapping = {\n",
    "                'Open': 'Open',\n",
    "                'High': 'High',\n",
    "                'Low': 'Low',\n",
    "                'Close': 'Close',\n",
    "                'Adj Close': 'Adj_Close',\n",
    "                'AdjClose': 'Adj_Close',  # Alternative naming\n",
    "                'Volume': 'Volume'\n",
    "            }\n",
    "\n",
    "            # Rename columns to standard names\n",
    "            for old_name, new_name in column_mapping.items():\n",
    "                if old_name in df.columns:\n",
    "                    df = df.rename(columns={old_name: new_name})\n",
    "                    print(f\"{symbol} - Renamed {old_name} to {new_name}\")\n",
    "\n",
    "            print(f\"{symbol} - Final columns before processing: {list(df.columns)}\")\n",
    "\n",
    "            # Calculate additional metrics\n",
    "            if 'Close' in df.columns:\n",
    "                df['Close_Change'] = df['Close'].diff().fillna(0)\n",
    "                df['Close_Pct_Change'] = (df['Close'].pct_change().fillna(0) * 100)\n",
    "\n",
    "                # Calculate daily range\n",
    "                if 'High' in df.columns and 'Low' in df.columns:\n",
    "                    df['Daily_Range'] = df['High'] - df['Low']\n",
    "                    df['Daily_Range_Pct'] = ((df['High'] - df['Low']) / df['Low'] * 100).fillna(0)\n",
    "\n",
    "            # Ensure all numeric columns are properly formatted\n",
    "            numeric_cols = ['Open', 'High', 'Low', 'Close', 'Volume',\n",
    "                           'Close_Change', 'Close_Pct_Change', 'Daily_Range', 'Daily_Range_Pct']\n",
    "\n",
    "            for col in numeric_cols:\n",
    "                if col in df.columns:\n",
    "                    df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "                    # Round to appropriate decimal places\n",
    "                    if col in ['Close_Change', 'Close_Pct_Change', 'Daily_Range', 'Daily_Range_Pct']:\n",
    "                        df[col] = df[col].round(4)\n",
    "                    else:\n",
    "                        df[col] = df[col].round(2)\n",
    "\n",
    "            # Handle missing values properly (don't fill with 0, use NULL)\n",
    "            df = df.replace([pd.NA, pd.NaT], None)\n",
    "\n",
    "            # Remove rows where essential data is missing\n",
    "            df = df.dropna(subset=['Date', 'Symbol', 'Close'])\n",
    "\n",
    "            # Define final column order for clean CSV (matching Snowflake table schema)\n",
    "            final_columns = ['Date', 'Symbol', 'Open', 'High', 'Low', 'Close',\n",
    "                           'Volume', 'Close_Change', 'Close_Pct_Change', 'Daily_Range', 'Daily_Range_Pct']\n",
    "\n",
    "            # Only select columns that exist and have data\n",
    "            available_columns = [col for col in final_columns if col in df.columns]\n",
    "            df = df[available_columns]\n",
    "\n",
    "            if not df.empty:\n",
    "                all_data.append(df)\n",
    "                print(f\"{symbol}: {len(df)} rows, columns: {list(df.columns)}\")\n",
    "            else:\n",
    "                print(f\"{symbol}: No valid data after processing\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {symbol}: {str(e)}\")\n",
    "            print(f\"Error type: {type(e).__name__}\")\n",
    "            import traceback\n",
    "            print(f\"Full traceback: {traceback.format_exc()}\")\n",
    "            continue\n",
    "\n",
    "    if not all_data:\n",
    "        raise ValueError(\"No data fetched for any symbols!\")\n",
    "\n",
    "    # Combine all data\n",
    "    final_df = pd.concat(all_data, ignore_index=True)\n",
    "\n",
    "    # Sort by date and symbol for organized data\n",
    "    final_df = final_df.sort_values(['Date', 'Symbol']).reset_index(drop=True)\n",
    "\n",
    "    # Data quality checks\n",
    "    print(f\"\\nData Quality Summary:\")\n",
    "    print(f\"Total rows: {len(final_df)}\")\n",
    "    print(f\"Unique symbols: {final_df['Symbol'].nunique()}\")\n",
    "    print(f\"Date range: {final_df['Date'].min()} to {final_df['Date'].max()}\")\n",
    "    print(f\"Missing values per column:\")\n",
    "    for col in final_df.columns:\n",
    "        missing_count = final_df[col].isnull().sum()\n",
    "        if missing_count > 0:\n",
    "            print(f\"  {col}: {missing_count}\")\n",
    "\n",
    "    # Generate CSV filename with timestamp\n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    csv_path = f\"/tmp/sp500_data_{timestamp}.csv\"\n",
    "\n",
    "    # Save with enhanced CSV formatting\n",
    "    final_df.to_csv(\n",
    "        csv_path,\n",
    "        index=False,\n",
    "        quoting=1,  # QUOTE_ALL for Snowflake compatibility\n",
    "        encoding='utf-8',\n",
    "        float_format='%.4f',  # More precision for financial data\n",
    "        na_rep='',  # Empty string for NULL values\n",
    "        date_format='%Y-%m-%d'\n",
    "    )\n",
    "\n",
    "    context['ti'].xcom_push(key='csv_path', value=csv_path)\n",
    "    context['ti'].xcom_push(key='row_count', value=len(final_df))\n",
    "    context['ti'].xcom_push(key='symbol_count', value=final_df['Symbol'].nunique())\n",
    "\n",
    "    print(f\"\\nData saved to {csv_path}\")\n",
    "    print(f\"File size: {os.path.getsize(csv_path) / 1024:.2f} KB\")\n",
    "\n",
    "    # Show data preview\n",
    "    print(f\"\\nData Preview (first 3 rows):\")\n",
    "    print(final_df.head(3).to_string(index=False))\n",
    "\n",
    "    return csv_path\n",
    "\n",
    "\n",
    "def save_and_upload(**context):\n",
    "    \"\"\"Upload CSV to S3 with enhanced error handling\"\"\"\n",
    "    csv_path = context['ti'].xcom_pull(key='csv_path', task_ids='get_stock_data')\n",
    "    row_count = context['ti'].xcom_pull(key='row_count', task_ids='get_stock_data')\n",
    "    symbol_count = context['ti'].xcom_pull(key='symbol_count', task_ids='get_stock_data')\n",
    "\n",
    "    if not csv_path:\n",
    "        raise ValueError(\"No CSV path received from previous task\")\n",
    "\n",
    "    if not os.path.exists(csv_path):\n",
    "        raise FileNotFoundError(f\"CSV file not found: {csv_path}\")\n",
    "\n",
    "    file_name = os.path.basename(csv_path)\n",
    "    file_size = os.path.getsize(csv_path)\n",
    "\n",
    "    print(f\"Uploading file: {file_name}\")\n",
    "    print(f\"File size: {file_size / 1024:.2f} KB\")\n",
    "    print(f\"Rows: {row_count}, Symbols: {symbol_count}\")\n",
    "\n",
    "    try:\n",
    "        s3_hook = S3Hook(aws_conn_id='aws_default')  # Change to your AWS connection ID\n",
    "\n",
    "        # Upload to S3\n",
    "        s3_hook.load_file(\n",
    "            filename=csv_path,\n",
    "            key=f\"{S3_FOLDER}/{file_name}\",\n",
    "            bucket_name=S3_BUCKET,\n",
    "            replace=True\n",
    "        )\n",
    "\n",
    "        print(f\"Successfully uploaded to s3://{S3_BUCKET}/{S3_FOLDER}/{file_name}\")\n",
    "\n",
    "        # Store S3 path for Snowflake task\n",
    "        s3_path = f\"s3://{S3_BUCKET}/{S3_FOLDER}/{file_name}\"\n",
    "        context['ti'].xcom_push(key='s3_path', value=s3_path)\n",
    "        context['ti'].xcom_push(key='file_name', value=file_name)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"S3 upload failed: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "    # Clean up temp file\n",
    "    try:\n",
    "        os.remove(csv_path)\n",
    "        print(f\"Cleaned up temp file: {csv_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Could not delete temp file: {e}\")\n",
    "\n",
    "\n",
    "# -----------------------------------------\n",
    "# Default args & DAG definition\n",
    "# -----------------------------------------\n",
    "default_args = {\n",
    "    'owner': 'data_team',\n",
    "    'start_date': datetime(2025, 1, 1),\n",
    "    'retries': 2,\n",
    "    'retry_delay': timedelta(minutes=5),\n",
    "    'email_on_failure': False,\n",
    "    'email_on_retry': False,\n",
    "    'depends_on_past': False,\n",
    "}\n",
    "\n",
    "with DAG(\n",
    "    dag_id='sp500_pipeline_dag',\n",
    "    default_args=default_args,\n",
    "    description='S&P 500 Stock Data Pipeline - Extract, Transform, Load to S3 and Snowflake',\n",
    "    schedule='0 0 * * *',  # Runs every day at midnight\n",
    "    catchup=False,\n",
    "    max_active_runs=1,\n",
    "    tags=['stocks', 'sp500', 'aws', 'snowflake', 'etl', 'financial_data']\n",
    ") as dag:\n",
    "\n",
    "    start = EmptyOperator(task_id='start')\n",
    "\n",
    "    extract_symbols_task = PythonOperator(\n",
    "        task_id='extract_symbols',\n",
    "        python_callable=extract_symbols,\n",
    "    )\n",
    "\n",
    "    get_stock_data_task = PythonOperator(\n",
    "        task_id='get_stock_data',\n",
    "        python_callable=get_stock_data,\n",
    "    )\n",
    "\n",
    "    save_and_upload_task = PythonOperator(\n",
    "        task_id='save_and_upload',\n",
    "        python_callable=save_and_upload,\n",
    "    )\n",
    "\n",
    "    # Load data task - simplified since table and stage already exist\n",
    "    load_to_snowflake_task = SQLExecuteQueryOperator(\n",
    "        task_id='load_to_snowflake',\n",
    "        conn_id='snowflake_default',\n",
    "        sql=\"\"\"\n",
    "        -- Copy data from S3 to Snowflake table using existing stage\n",
    "        COPY INTO STOCK_DATA (\n",
    "            DATE,\n",
    "            SYMBOL,\n",
    "            OPEN,\n",
    "            HIGH,\n",
    "            LOW,\n",
    "            CLOSE,\n",
    "            VOLUME,\n",
    "            CLOSE_CHANGE,\n",
    "            CLOSE_PCT_CHANGE,\n",
    "            DAILY_RANGE,\n",
    "            DAILY_RANGE_PCT\n",
    "        )\n",
    "        FROM @my_s3_stage\n",
    "        FILE_FORMAT = (\n",
    "            TYPE = 'CSV'\n",
    "            FIELD_OPTIONALLY_ENCLOSED_BY = '\"'\n",
    "            SKIP_HEADER = 1\n",
    "            NULL_IF = ('NULL', 'null', '', '\\\\N')\n",
    "            FIELD_DELIMITER = ','\n",
    "            TRIM_SPACE = TRUE\n",
    "            ERROR_ON_COLUMN_COUNT_MISMATCH = FALSE\n",
    "            EMPTY_FIELD_AS_NULL = TRUE\n",
    "        )\n",
    "        ON_ERROR = 'CONTINUE'\n",
    "        PURGE = TRUE;\n",
    "\n",
    "        -- Verify the load\n",
    "        SELECT\n",
    "            COUNT(*) as total_rows,\n",
    "            COUNT(DISTINCT SYMBOL) as unique_symbols,\n",
    "            MIN(DATE) as earliest_date,\n",
    "            MAX(DATE) as latest_date,\n",
    "        FROM STOCK_DATA;\n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "    end = EmptyOperator(task_id='end')\n",
    "\n",
    "    # DAG dependencies\n",
    "    start >> extract_symbols_task >> get_stock_data_task >> save_and_upload_task >> load_to_snowflake_task >> end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb65f67-0eea-4ca8-a7e2-be63146ea581",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
